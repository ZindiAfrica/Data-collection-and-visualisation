{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UYNvAExBmKR"
      },
      "source": [
        "# Hack the Continent Open Buildings Challenge\n",
        "\n",
        "## A study on the use of google open building data to extrapolate Gauteng's quality of life data to the rest of South Africa.\n",
        "\n",
        "This notebook is a study of how to extrapolate quality of living survey (QOLS) data using Google open buildings (GOB) data and the relative wealth index (RWI) produced from data made available by Meta. Google used image recognition on satellite images to produce the open building data set.\n",
        "I investigate how the average building size, as well as Google's trust in the data, correlated to different fields in the QOLS data set.\n",
        "In the learning and forecasting model, I also included a relative wealth index that was calculated previously for the entire country.  \n",
        "The quality of life survey only covers Gauteng, the model is thus trained using Gauteng data and then used to predict values for the rest of South Africa.\n",
        "\n",
        "The RWI data is calculated for a grid size of 2.4km. The GOB data has a latitude and longitude for each building detected.\n",
        "As the QOLS data had an aim of gathering a minimum amount of data points per ward, it was deemed that using wards as the granularity of calculation was the most practical approach to split the area. Descriptive statistics were calculated for each of the data set, per ward, and then the data sets were merged to generate a learning data set with ward level granularity.  \n",
        "  \n",
        "  \n",
        "The data sets used are:  \n",
        "QOLS - Quality of Life Survey (\n",
        "https://www.gcro.ac.za/research/project/detail/quality-of-life-survey-v-201718/\n",
        ")  \n",
        "\n",
        "GOB - Google Open Buildings](https://sites.research.google/open-buildings/)  \n",
        "RWI - Relative Wealth Index - Microestimates of wealth for all low- and middle-income countries Guanghua Chi, Han Fang, Sourav Chatterjee, Joshua E. Blumenstock Proceedings of the National Academy of Sciences Jan 2022, 119 (3) e2113658119; DOI: 10.1073/pnas.2113658119 (https://data.humdata.org/dataset/76f2a2ea-ba50-40f5-b79c-db95d668b843/resource/66567fcf-98a0-4246-a799-6d4616633228/download/zaf_relative_wealth_index.csv)   \n",
        "\n",
        "Municipal ward boundaries: https://dataportal-mdb-sa.opendata.arcgis.com/datasets/e0223a825ea2481fa72220ad3204276b/about\n",
        "\n",
        "  \n",
        "This whole project will be made available on github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eXL156ae-iT"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I have downloaded the data sets to my personal Google drive for quicker access. \n",
        "\n",
        "The below simply connects to my google drive, list the files, and creates a variable to represent the path to input and output data."
      ],
      "metadata": {
        "id": "8PNFqhD4X5TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5hC32t_dZ35R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gauteng = gpd.read_file()\n",
        "!ls /content/drive"
      ],
      "metadata": {
        "id": "3_VvKhHUS8A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "rawdatadir = \"/content/drive/MyDrive/Colab Notebooks/ZindiHackAfrica/01_raw_data\"\n",
        "preprocesseddir = \"/content/drive/MyDrive/Colab Notebooks/ZindiHackAfrica/03_preprocessed_data/\"\n",
        "os.listdir(rawdatadir)"
      ],
      "metadata": {
        "id": "2sY9kBPcTejW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import the required libraries."
      ],
      "metadata": {
        "id": "ohc58JID3Mxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install s2geometry pygeos geopandas\n",
        "\n",
        "import functools\n",
        "import glob\n",
        "import gzip\n",
        "import multiprocessing\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "\n",
        "import gdal\n",
        "import geopandas as gpd\n",
        "from google.colab import files\n",
        "from IPython import display\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import s2geometry as s2\n",
        "import shapely\n",
        "import tensorflow as tf\n",
        "import tqdm.notebook\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pTI0F-74T3ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### I want to be able to scroll across all the field, particularly in the qols data set so I remove the limit on the number of columns displayed."
      ],
      "metadata": {
        "id": "Rv0uKV3DZPLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_columns = None"
      ],
      "metadata": {
        "id": "wFvBhTfM2hde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "S4JFVVkbio2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "opening the data sets for pre-processing, The raw GOB data is not stored locally, so not in the list below but we'll get to it further along."
      ],
      "metadata": {
        "id": "ZwYsfh3Sa3q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rsa = gpd.read_file(rawdatadir + '/stanford-nt680nw4634-geojson.json')  # geometries for south African municipal areas... seems kinda old, eg reference to \"northern province\"\n",
        "rwi = gpd.read_file(rawdatadir + '/zaf_relative_wealth_index.csv')  # relative wealth index\n",
        "#allpoi = gpd.read_file(f'zip://{rawdatadir}/hotosm_zaf_points_of_interest_points_shp.zip')  # points of interest schools etc\n",
        "#financialpoi = gpd.read_file(f'zip://{rawdatadir}/hotosm_zaf_financial_services_points_shp.zip') # points of interest financial \n",
        "qols = gpd.read_file(f'{rawdatadir}/qols-v-2017-2018-v1.1.csv') # quality of life standards from 2017 - 2018\n",
        "wards = gpd.read_file(f'zip://{rawdatadir}/SA_Wards2020.zip') # South African Ward boudaries"
      ],
      "metadata": {
        "id": "3bZl9IHlb_oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look at the data gathered in the QOLS survey, its pretty awesome, there's a lot here that could be invetigated."
      ],
      "metadata": {
        "id": "_6uP0WPsb-tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qols.head(3)"
      ],
      "metadata": {
        "id": "YU4Z3JJLdCIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just a sanity check that the QOLS data has the same number of wards for Gauteng as the wards data set.  \n",
        "Interesting to note that there are 529 wards in our training set that more than I expected, and it give a better training set size are more granularity than I thought. Nice!"
      ],
      "metadata": {
        "id": "GYsjJyz5zr3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "qolsnumwards = len(qols['ward'].unique())\n",
        "numwards = len(wards[wards['Province'] == 'Gauteng']['WardID'].unique())\n",
        "print(f'number of wards in the quality of life survey data set: {qolsnumwards}')\n",
        "print(f'number of wards according to arcgis 2020: {numwards}')"
      ],
      "metadata": {
        "id": "Dyvl266PtV6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose two fields to focus on for this work. One that I expect should correlate well to the GOB data and one that I expect would not correlate well. They are the number of rooms in the household and the length of stay in the current household.  \n",
        "below I check the unique values in each of the two files."
      ],
      "metadata": {
        "id": "4vSI0_DccPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Values in the rooms field: ', (qols['Q1_02_rooms'].unique()))\n",
        "print('Values in the length of stay field: ', qols['Q4_04_lengthof_stay'].unique())"
      ],
      "metadata": {
        "id": "jHiJ0k8uczw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of rooms is numeric except the 10+, it is thus replaced by '10' and then cast to an int.  \n",
        "The length of stay cannot be cast to a numeric value easily and thus is mapped with a dictionary lookup."
      ],
      "metadata": {
        "id": "9i71SstKxykd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maplengthof_stay = {'More than 10 years' : 10, '3-4 years': 4, \"I've always lived here\": 25,\n",
        "       'Less than 1 year':1, '1-2 years':2, '5-10 years':8}\n",
        "\n",
        "qolstoy = qols[['ward', 'Q4_04_lengthof_stay', 'Q1_02_rooms']].copy()\n",
        "qolstoy.replace(maplengthof_stay, inplace = True)\n",
        "qolstoy.replace({'10+':'10'}, inplace = True)\n",
        "qolstoy['Q1_02_rooms']=qolstoy['Q1_02_rooms'].astype(int)\n",
        "\n",
        "qolstoy = qolstoy.groupby('ward').agg(['median','min', 'max', 'std'])"
      ],
      "metadata": {
        "id": "pPP4J7D1i-t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wards.set_geometry('geometry', drop=True, inplace=True, crs='EPSG:4326')\n",
        "wards"
      ],
      "metadata": {
        "id": "xfL5cNZwv8uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RWI data needs to be reformatted and aggregated per ward."
      ],
      "metadata": {
        "id": "PMnRPORbyoox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rwi['geometry'] =gpd.points_from_xy(rwi['longitude'], rwi['latitude'])\n",
        "rwi = gpd.GeoDataFrame(rwi, crs=\"EPSG:4326\")\n",
        "\n",
        "points = wards.sjoin(rwi)\n",
        "\n",
        "rwiward = points[['WardID','rwi','error']].groupby('WardID').agg(['median','min', 'max', 'std'])\n",
        "rwiward"
      ],
      "metadata": {
        "id": "AG_C58xuxrBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the RWI data can then be joined onto the QOLS data set per ward."
      ],
      "metadata": {
        "id": "5iiP5n9Fy-L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rwiqolward = rwiward.merge(qolstoy, left_index = True, right_index = True, how = 'left')"
      ],
      "metadata": {
        "id": "I3M1H43qvNMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the boundaries of the wards data. just to get a feel for what they look like. We can see that wards seem to be smaller where there are more dense populations, also there are a lot of wards in the South Africa.\n",
        "\n"
      ],
      "metadata": {
        "id": "VP0nj1w80tKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wards.boundary.plot()"
      ],
      "metadata": {
        "id": "oWruoziWhE6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code that is used to get the data from Googles Open building data, I used it as supplied in the example notebook with some minor changes."
      ],
      "metadata": {
        "id": "UBukQWtJ0Jv_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbGgdE4mj1hd"
      },
      "source": [
        "### Download buildings data for a region in Africa [takes up to 15 minutes for large countries]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP6ADuzRdZTF"
      },
      "outputs": [],
      "source": [
        "#@markdown Select a region from either the [Natural Earth low res](https://www.naturalearthdata.com/downloads/110m-cultural-vectors/110m-admin-0-countries/) (fastest), [Natural Earth high res](https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/) or [World Bank high res](https://datacatalog.worldbank.org/dataset/world-bank-official-boundaries) shapefiles:\n",
        "region_border_source = 'Natural Earth (Low Res 110m)'  #@param [\"Natural Earth (Low Res 110m)\", \"Natural Earth (High Res 10m)\", \"World Bank (High Res 10m)\"]\n",
        "region = 'ZAF (South Africa)'  #@param [\"\", \"AGO (Angola)\", \"BDI (Burundi)\", \"BEN (Benin)\", \"BFA (Burkina Faso)\", \"BWA (Botswana)\", \"CAF (Central African Republic)\", \"CIV (CÃ´te d'Ivoire)\", \"COD (Democratic Republic of the Congo)\", \"COG (Republic of the Congo)\", \"DJI (Djibouti)\", \"DZA (Algeria)\", \"EGY (Egypt)\", \"ERI (Eritrea)\", \"ETH (Ethiopia)\", \"GAB (Gabon)\", \"GHA (Ghana)\", \"GIN (Guinea)\", \"GMB (The Gambia)\", \"GNB (Guinea-Bissau)\", \"GNQ (Equatorial Guinea)\", \"KEN (Kenya)\", \"LBR (Liberia)\", \"LSO (Lesotho)\", \"MDG (Madagascar)\", \"MOZ (Mozambique)\", \"MRT (Mauritania)\", \"MWI (Malawi)\", \"NAM (Namibia)\", \"NER (Niger)\", \"NGA (Nigeria)\", \"RWA (Rwanda)\", \"SDN (Sudan)\", \"SEN (Senegal)\", \"SLE (Sierra Leone)\", \"SOM (Somalia)\", \"SWZ (eSwatini)\", \"TGO (Togo)\", \"TUN (Tunisia)\", \"TZA (Tanzania)\", \"UGA (Uganda)\", \"ZAF (South Africa)\", \"ZMB (Zambia)\", \"ZWE (Zimbabwe)\"]\n",
        "province = \"\" #@param [\"NORTHERN CAPE\", \"NORTHERN PROVINCE\", \"MPUMALANGA\", \"NORTH WEST\", \"GAUTENG\", \"FREE STATE\", \"KWAZULU-NATAL\", \"EASTERN CAPE\", \"WESTERN CAPE\", \"\"]\n",
        "\n",
        "# @markdown Alternatively, specify an area of interest in [WKT format](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) (assumes crs='EPSG:4326'); this [tool](https://arthur-e.github.io/Wicket/sandbox-gmaps3.html) might be useful.\n",
        "your_own_wkt_polygon = ''  #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "BUILDING_DOWNLOAD_PATH = ('gs://open-buildings-data/v1/'\n",
        "                          'polygons_s2_level_6_gzip_no_header')\n",
        "\n",
        "def get_filename_and_region_dataframe(\n",
        "    region_border_source: str, region: str,\n",
        "    your_own_wkt_polygon: str) -> Tuple[str, gpd.geodataframe.GeoDataFrame]:\n",
        "  \"\"\"Returns output filename and a geopandas dataframe with one region row.\"\"\"\n",
        "\n",
        "  if your_own_wkt_polygon:\n",
        "    filename = 'open_buildings_v1_polygons_your_own_wkt_polygon.csv.gz'\n",
        "    region_df = gpd.GeoDataFrame(\n",
        "        geometry=gpd.GeoSeries.from_wkt([your_own_wkt_polygon]),\n",
        "        crs='EPSG:4326')\n",
        "    if not isinstance(region_df.iloc[0].geometry,\n",
        "                      shapely.geometry.polygon.Polygon) and not isinstance(\n",
        "                          region_df.iloc[0].geometry,\n",
        "                          shapely.geometry.multipolygon.MultiPolygon):\n",
        "      raise ValueError(\"`your_own_wkt_polygon` must be a POLYGON or \"\n",
        "                      \"MULTIPOLYGON.\")\n",
        "  elif province:\n",
        "    region_df = gpd.GeoDataFrame(geometry=[rsa[(rsa['nam'] == province)].geometry.unary_union], crs=rsa.crs)\n",
        "    filename = f'open_buildings_v1_polygons_{province}.csv.gz'\n",
        "    jhbpoly.plot()\n",
        "\n",
        "    print(f'Preparing your_own_wkt_polygon.')\n",
        "    return filename, region_df\n",
        "\n",
        "  if not region:\n",
        "    raise ValueError('Please select a region or set your_own_wkt_polygon.')\n",
        "\n",
        "  if region_border_source == 'Natural Earth (Low Res 110m)':\n",
        "    url = ('https://www.naturalearthdata.com/http//www.naturalearthdata.com/'\n",
        "           'download/110m/cultural/ne_110m_admin_0_countries.zip')\n",
        "    !wget -N {url}\n",
        "    display.clear_output()\n",
        "    region_shapefile_path = os.path.basename(url)\n",
        "    source_name = 'ne_110m'\n",
        "  elif region_border_source == 'Natural Earth (High Res 10m)':\n",
        "    url = ('https://www.naturalearthdata.com/http//www.naturalearthdata.com/'\n",
        "           'download/10m/cultural/ne_10m_admin_0_countries.zip')\n",
        "    !wget -N {url}\n",
        "    display.clear_output()\n",
        "    region_shapefile_path = os.path.basename(url)\n",
        "    source_name = 'ne_10m'\n",
        "  elif region_border_source == 'World Bank (High Res 10m)':\n",
        "    url = ('https://development-data-hub-s3-public.s3.amazonaws.com/ddhfiles/'\n",
        "           '779551/wb_countries_admin0_10m.zip')\n",
        "    !wget -N {url}\n",
        "    !unzip -o {os.path.basename(url)}\n",
        "    display.clear_output()\n",
        "    region_shapefile_path = 'WB_countries_Admin0_10m'\n",
        "    source_name = 'wb_10m'\n",
        "\n",
        "  region_iso_a3 = region.split(' ')[0]\n",
        "  filename = f'open_buildings_v1_polygons_{source_name}_{region_iso_a3}.csv.gz'\n",
        "  region_df = gpd.read_file(region_shapefile_path).query(\n",
        "      f'ISO_A3 == \"{region_iso_a3}\"').dissolve(by='ISO_A3')[['geometry']]\n",
        "  print(f'Preparing {region} from {region_border_source}.')\n",
        "  return filename, region_df\n",
        "\n",
        "\n",
        "def get_bounding_box_s2_covering_tokens(\n",
        "    region_geometry: shapely.geometry.base.BaseGeometry) -> List[str]:\n",
        "  region_bounds = region_geometry.bounds\n",
        "  s2_lat_lng_rect = s2.S2LatLngRect_FromPointPair(\n",
        "      s2.S2LatLng_FromDegrees(region_bounds[1], region_bounds[0]),\n",
        "      s2.S2LatLng_FromDegrees(region_bounds[3], region_bounds[2]))\n",
        "  coverer = s2.S2RegionCoverer()\n",
        "  # NOTE: Should be kept in-sync with s2 level in BUILDING_DOWNLOAD_PATH.\n",
        "  coverer.set_fixed_level(6)\n",
        "  coverer.set_max_cells(1000000)\n",
        "  return [cell.ToToken() for cell in coverer.GetCovering(s2_lat_lng_rect)]\n",
        "\n",
        "\n",
        "def s2_token_to_shapely_polygon(\n",
        "    s2_token: str) -> shapely.geometry.polygon.Polygon:\n",
        "  s2_cell = s2.S2Cell(s2.S2CellId_FromToken(s2_token, len(s2_token)))\n",
        "  coords = []\n",
        "  for i in range(4):\n",
        "    s2_lat_lng = s2.S2LatLng(s2_cell.GetVertex(i))\n",
        "    coords.append((s2_lat_lng.lng().degrees(), s2_lat_lng.lat().degrees()))\n",
        "  return shapely.geometry.Polygon(coords)\n",
        "\n",
        "\n",
        "def download_s2_token(\n",
        "    s2_token: str, region_df: gpd.geodataframe.GeoDataFrame) -> Optional[str]:\n",
        "  \"\"\"Downloads the matching CSV file with polygons for the `s2_token`.\n",
        "\n",
        "  NOTE: Only polygons inside the region are kept.\n",
        "  NOTE: Passing output via a temporary file to reduce memory usage.\n",
        "\n",
        "  Args:\n",
        "    s2_token: S2 token for which to download the CSV file with building\n",
        "      polygons. The S2 token should be at the same level as the files in\n",
        "      BUILDING_DOWNLOAD_PATH.\n",
        "    region_df: A geopandas dataframe with only one row that contains the region\n",
        "      for which to keep polygons.\n",
        "\n",
        "  Returns:\n",
        "    Either filepath which contains a gzipped CSV without header for the\n",
        "    `s2_token` subfiltered to only contain building polygons inside the region\n",
        "    or None which means that there were no polygons inside the region for this\n",
        "    `s2_token`.\n",
        "  \"\"\"\n",
        "  s2_cell_geometry = s2_token_to_shapely_polygon(s2_token)\n",
        "  region_geometry = region_df.iloc[0].geometry\n",
        "  prepared_region_geometry = shapely.prepared.prep(region_geometry)\n",
        "  # If the s2 cell doesn't intersect the country geometry at all then we can\n",
        "  # know that all rows would be dropped so instead we can just return early.\n",
        "  if not prepared_region_geometry.intersects(s2_cell_geometry):\n",
        "    return None\n",
        "  try:\n",
        "    # Using tf.io.gfile.GFile gives better performance than passing the GCS path\n",
        "    # directly to pd.read_csv.\n",
        "    with tf.io.gfile.GFile(\n",
        "        os.path.join(BUILDING_DOWNLOAD_PATH, f'{s2_token}_buildings.csv.gz'),\n",
        "        'rb') as gf:\n",
        "      # If the s2 cell is fully covered by country geometry then can skip\n",
        "      # filtering as we need all rows.\n",
        "      if prepared_region_geometry.covers(s2_cell_geometry):\n",
        "        with tempfile.NamedTemporaryFile(mode='w+b', delete=False) as tmp_f:\n",
        "          shutil.copyfileobj(gf, tmp_f)\n",
        "          return tmp_f.name\n",
        "      # Else take the slow path.\n",
        "      # NOTE: We read in chunks to save memory.\n",
        "      csv_chunks = pd.read_csv(\n",
        "          gf, chunksize=2000000, dtype=object, compression='gzip', header=None)\n",
        "      tmp_f = tempfile.NamedTemporaryFile(mode='w+b', delete=False)\n",
        "      tmp_f.close()\n",
        "      for csv_chunk in csv_chunks:\n",
        "        points = gpd.GeoDataFrame(\n",
        "            geometry=gpd.points_from_xy(csv_chunk[1], csv_chunk[0]),\n",
        "            crs='EPSG:4326')\n",
        "        # sjoin 'within' was faster than using shapely's 'within' directly.\n",
        "        points = gpd.sjoin(points, region_df, predicate='within')\n",
        "        csv_chunk = csv_chunk.iloc[points.index]\n",
        "        csv_chunk.to_csv(\n",
        "            tmp_f.name,\n",
        "            mode='ab',\n",
        "            index=False,\n",
        "            header=False,\n",
        "            compression={\n",
        "                'method': 'gzip',\n",
        "                'compresslevel': 1\n",
        "            })\n",
        "      return tmp_f.name\n",
        "  except tf.errors.NotFoundError:\n",
        "    return None\n",
        "\n",
        "# Clear output after pip install.\n",
        "display.clear_output()\n",
        "filename, region_df = get_filename_and_region_dataframe(region_border_source,\n",
        "                                                        region,\n",
        "                                                        your_own_wkt_polygon)\n",
        "# Remove any old outputs to not run out of disk.\n",
        "for f in glob.glob('/tmp/open_buildings_*'):\n",
        "  os.remove(f)\n",
        "# Write header to the compressed CSV file.\n",
        "with gzip.open(f'/tmp/{filename}', 'wt') as merged:\n",
        "  merged.write(','.join([\n",
        "      'latitude', 'longitude', 'area_in_meters', 'confidence', 'geometry',\n",
        "      'full_plus_code'\n",
        "  ]) + '\\n')\n",
        "download_s2_token_fn = functools.partial(download_s2_token, region_df=region_df)\n",
        "s2_tokens = get_bounding_box_s2_covering_tokens(region_df.iloc[0].geometry)\n",
        "# Downloads CSV files for relevant S2 tokens and after filtering appends them\n",
        "# to the compressed output CSV file. Relies on the fact that concatenating\n",
        "# gzipped files produces a valid gzip file.\n",
        "# NOTE: Uses a pool to speed up output preparation.\n",
        "with open(f'/tmp/{filename}', 'ab') as merged:\n",
        "  with multiprocessing.Pool(4) as e:\n",
        "    for fname in tqdm.notebook.tqdm(\n",
        "        e.imap_unordered(download_s2_token_fn, s2_tokens),\n",
        "        total=len(s2_tokens)):\n",
        "      if fname:\n",
        "        with open(fname, 'rb') as tmp_f:\n",
        "          shutil.copyfileobj(tmp_f, merged)\n",
        "        os.unlink(fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY9Ba1Cxpcb4"
      },
      "source": [
        "# Visualise the data\n",
        "\n",
        "First we convert the CSV file into a GeoDataFrame. The CSV files can be quite large because they include the polygon outline of every building. For this example we only need longitude and latitude, so we only process those columns to save memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSpb1JKVjuYj"
      },
      "outputs": [],
      "source": [
        "buildings = pd.read_csv(\n",
        "    f\"/tmp/{filename}\", engine=\"c\",\n",
        "    usecols=['latitude', 'longitude', 'area_in_meters', 'confidence'])\n",
        "\n",
        "print(f\"Read {len(buildings):,} records.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I write the GOB data and the rwi+qols data to file so they can be opened more quickly in future. They are commented out for now as I have the data on file and I do not want to accidentally overwrite it."
      ],
      "metadata": {
        "id": "4MMlvMhb1j2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rwiqolward.to_csv(preprocesseddir + 'rwiqolward.csv')  # geometries for south African municipal areas... seems kinda old, eg reference to \"northern province\"\n",
        "#buildings.to_csv(preprocesseddir + 'buildings.csv')  # relative wealth index"
      ],
      "metadata": {
        "id": "lfXSOWZv1Dcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open preprocessed data and do learning\n"
      ],
      "metadata": {
        "id": "Mbl-BoB8ooTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rwiqolward = pd.read_csv(preprocesseddir + 'rwiqolward.csv', header=[0,1,2]) "
      ],
      "metadata": {
        "id": "PtCbmGKScNiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Buildings data set is really big, and trying to aggregate it per ward means my free colab machine runs out of ram and falls over.  \n",
        "Here I find the extremes of the latitude and longitude for each ward and filter the building data on that before filtering it on the geometry of the ward.\n",
        "I then aggregate the building data for the ward.  \n",
        "This is pretty time consuming and I do not want to do it every time so I write the output to file."
      ],
      "metadata": {
        "id": "sZ7RXBP42OlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the descriptive statistics for building area per ward.\n",
        "buildings = pd.read_csv(preprocesseddir + 'buildings.csv')\n",
        "outlist = []\n",
        "for wardno in wards['WardID'].unique():\n",
        "  eachward = wards[wards['WardID'] == wardno]\n",
        "  minx,miny,maxx,maxy = eachward.bounds.iloc[0]\n",
        "  buildingswards = buildings[(buildings['longitude'] < maxx) &(buildings['longitude'] > minx) &(buildings['latitude'] > miny) &(buildings['latitude'] < maxy)]\n",
        "  buildingswards = gpd.GeoDataFrame(buildingswards,\n",
        "        geometry=gpd.points_from_xy(buildingswards.longitude,\n",
        "                                    buildingswards.latitude),crs = \"EPSG:4326\")\n",
        "  buildingswards = gpd.sjoin(eachward, buildingswards, how=\"left\")\n",
        "  bwaggs = buildingswards[['area_in_meters',\t'confidence']].agg(['median','min','max','std','count'])\n",
        "  dfout = pd.DataFrame([{'wardID': wardno}])\n",
        "  dfout[['Amedian','Amin','Amax','Astd','Acount','Cmedian','Cmin','Cmax','Cstd','Ccount']] = bwaggs['area_in_meters'].append(bwaggs['confidence'])#.append(wardno)\n",
        "  outlist.append(dfout)\n",
        "dfdone = pd.concat(outlist)\n",
        "dfdone.to_csv(f'{preprocesseddir}dfbuildingstats.csv')\n"
      ],
      "metadata": {
        "id": "s9IchgrkhoZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfdone= pd.read_csv(f'{preprocesseddir}dfbuildingstats.csv')"
      ],
      "metadata": {
        "id": "6RJ0_FbOpYr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfdone['wardID'] = dfdone['wardID'].astype(int)"
      ],
      "metadata": {
        "id": "WvRX1jjEnHjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rwiqolward.shape)\n",
        "rwiqolward.head()"
      ],
      "metadata": {
        "id": "CsYBBcJapD2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above we can see that the headings from my rwiqolsward data set are a mess so I cleaned them up a little."
      ],
      "metadata": {
        "id": "b9R-j1Oz4AYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['wardID'] + [' '.join(col).strip().split(' Unnamed:')[0] for col in rwiqolward.columns.values][1:]\n",
        "rwiqolward.columns = cols"
      ],
      "metadata": {
        "id": "mtXxhzCtY9hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging the rwiqolsward data with the building data(dfdone) gets me the cleaned data set that I want to use for learning and prediction.\n",
        "\n",
        "# Model1: Learn the number of rooms in a house hold."
      ],
      "metadata": {
        "id": "TSY5fM5n4TWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfrooms = dfdone.merge(rwiqolward[['wardID','rwi median','rwi min','rwi max','rwi std','error median','error min','error max','error std','Q1_02_rooms median']], on = 'wardID', how = 'left').copy()"
      ],
      "metadata": {
        "id": "24N15MDppVsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfrooms.head()"
      ],
      "metadata": {
        "id": "DsQ4pS_U4oaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this data set is for all of South Africa, but I only have qols data for Gauteng, I split this data in two: the learning data for Gauteng, and the predicting data for the rest of the country."
      ],
      "metadata": {
        "id": "KQNVnES0401D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfqolspred = dfrooms[dfrooms['Q1_02_rooms median'].isna()].copy()\n",
        "dfqolslearn = dfrooms[~dfrooms['Q1_02_rooms median'].isna()]"
      ],
      "metadata": {
        "id": "no-0kCJdaqGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the data for a little machine learning."
      ],
      "metadata": {
        "id": "iLSPmUGw5M5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = dfqolslearn#.values\n",
        "# split data into input and output columns\n",
        "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
        "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "metadata": {
        "id": "Ou5CGdmwlw87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do a quick cross validation XGboost training exercise to see how well the data can be learned, we can see how many rounds are needed to have good learning. And give us a good idea if the model is fitting well or if its overfitting.\n",
        "This graph shows that the model learns well on this data the mean root mean square error is less that one, and the test and train plots track well together, we can see some over fitting as the rounds go on, buy at around 40 rounds the fit looks good and the over fitting looks minimal."
      ],
      "metadata": {
        "id": "VPrCfR7N56xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.1,'learning_rate': 0.08,\n",
        "                'max_depth': 8, 'alpha': 8}\n",
        "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
        "                    num_boost_round=100,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "cv_results[['train-rmse-mean','test-rmse-mean']].plot()"
      ],
      "metadata": {
        "id": "a0g9sXB6Nda9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FAv9fbUhnEIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross validation does not produce a model though so we need to train the model on our training data, we can train it on all the training data, as we are happy with the parameters from the cross validation output."
      ],
      "metadata": {
        "id": "qohFCFR1BS5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=40)"
      ],
      "metadata": {
        "id": "PliqNuYGlwya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGboots has a cool functions that shows us the importance of the most important features in the data set. Here we see that the standard deviation on the area of the building is the most important feature to the model, this is unexpected as I would have expected the median area to be the most useful. I am glad I checked, because this could mean that the learning was not a good as I thought when I looked at the test/train plot above."
      ],
      "metadata": {
        "id": "J20VmiORBrOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.plot_importance(xg_reg)\n",
        "plt.rcParams['figure.figsize'] = [5, 5]\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x6qiv4ywlwsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just for completeness I then use this model to predict the median number of rooms for every ward across the country."
      ],
      "metadata": {
        "id": "2l_BMFtXCa8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dmatrix = xgb.DMatrix(data=dfqolspred.iloc[:, :-1])\n",
        "preds = xg_reg.predict(pred_dmatrix)\n",
        "dfqolspred.loc[:,'Q1_02_rooms median'] = preds"
      ],
      "metadata": {
        "id": "bIp3PJ_hTBT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfqolspred.head()"
      ],
      "metadata": {
        "id": "HJK6gcN3-ROs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Model2: Learn the length of stay."
      ],
      "metadata": {
        "id": "xf1MsN73VfnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rwiqolward.head()"
      ],
      "metadata": {
        "id": "P9kPlJnlVwqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dflos = dfdone.merge(rwiqolward[['wardID','rwi median','rwi min','rwi max','rwi std','error median','error min','error max','error std','Q4_04_lengthof_stay median']], on = 'wardID', how = 'left')\n",
        "dflos.drop('wardID',axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "Xo2M9c8ZVr_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this data set is for all of South Africa, but I only have qols data for Gauteng, I split this data in two: the learning data for Gauteng, and the predicting data for the rest of the country."
      ],
      "metadata": {
        "id": "8lieo0RHJvtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfqolspred = dflos[dflos['Q4_04_lengthof_stay median'].isna()]\n",
        "dfqolslearn = dflos[~dflos['Q4_04_lengthof_stay median'].isna()]"
      ],
      "metadata": {
        "id": "c3ARaTq1Vr_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format the data for consumption by xgboost library"
      ],
      "metadata": {
        "id": "dE4l6OcHKCoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = dfqolslearn#.values\n",
        "# split data into input and output columns\n",
        "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
        "data_dmatrix = xgb.DMatrix(data=X,label=y)"
      ],
      "metadata": {
        "id": "lzp3V-TaVr_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train and test plot below shows that after about 10 rounds the model starts to over fit very badly, at the point of divergence of the plots the Root Mean Square Error is almost 5. Which is pretty bad considering our input range is 2-25."
      ],
      "metadata": {
        "id": "ZgRtC7L9LytI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.1,'learning_rate': 0.08,\n",
        "                'max_depth': 8, 'alpha': 8}\n",
        "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
        "                    num_boost_round=100,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "cv_results[['train-rmse-mean','test-rmse-mean']].plot()"
      ],
      "metadata": {
        "id": "KcupLzldVr_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I keep the same parameters used in the cross validation but limit the number of rounds to 12 for the training, to avoid over fitting."
      ],
      "metadata": {
        "id": "6syuLU0LPtDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=12)"
      ],
      "metadata": {
        "id": "k6_rP-PdE4bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having a look at the feature importance we see that the RWI max is the most important for this model, I can understand that there is a correlation between the wealth of an area and how long people have lived there. There is some learning happening here but I am not confident that the predictions created by this model would actually be good enough to be valuable, so I am not going to extrapolate this for the rest of South Africa."
      ],
      "metadata": {
        "id": "aJIiDrDpP-J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.plot_importance(xg_reg)\n",
        "plt.rcParams['figure.figsize'] = [5, 5]\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zE4jub6kE4X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fGVd6z8BNMrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion \n",
        "\n",
        "The QOLS Data is awesome, its a good sample size ~23000 and covers a lot of detail. I have just scratched the surface on what is in it and there is a lot of scope to improve on what I have done here. One could look at other fields on the QOLS data, bring in other data sets to enrich the learning data set, and try different models.  \n",
        "In the set up for this project I have found many other data sets that may be useful to expand this project, but in the spirit of the Zindi Competition I wanted to use the GOB data and one other as a counter example.\n",
        "\n",
        "my list of other resource will be uploaded to my github in due course https://github.com/g-webber\n",
        "\n",
        "Thanks for reading."
      ],
      "metadata": {
        "id": "AKGFA8jsR8gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iLfSXTLNgw6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Open_buildings_spatial_analysis_examples.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}